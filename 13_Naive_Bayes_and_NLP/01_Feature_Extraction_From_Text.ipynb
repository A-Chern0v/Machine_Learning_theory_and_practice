{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72db67f5-510a-413b-bce0-10d9b8228dfa",
   "metadata": {},
   "source": [
    "# Извлечение признаков из текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5523ed-b850-4fb4-a2db-b575039d0f9f",
   "metadata": {},
   "source": [
    "Этот блокнот разделён на две части:\n",
    "* Для начала - выясним, что понадобится для построения инструментария NLP, который превратит набор текста в числовой массив из признаков. Для этого мы вручную вычислим, как часто встречаются те или иные слова, и построим метрику TF-IDF.\n",
    "* Далее выполним эти шаги с помощью Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d84e312-0460-4c62-a6aa-40e3ec4ce9cd",
   "metadata": {},
   "source": [
    "# Часть 1: основные принципы извлечения признаков из текста\n",
    "В этой части мы с помощью базовых операций Python построим очень простую систему NLP. Возьмём набор документов - это будет два текстовых файла. Далее создадим словарь из всех слов обоих документов. И затем посмотрим на технику **Мешок слов (Bag of Words)** для извлечения признаков из каждого документа.\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"margin: 20px\">В этом разделе мы только приводим иллюстрации основных принципов!\n",
    "<br>Не обращайте внимание на то, как пишется код в Python - позднее мы применим для этой задачи Scikit-Learn.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "033d85cc-05bb-4b32-af4a-3bb4394612a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e2c13f-a824-423c-b9a3-59f72b0f0b12",
   "metadata": {},
   "source": [
    "## Начнём с документов\n",
    "Для простоты изложения в наших текстовых файлах One.txt и Two.txt не будет каких-либо знаков пунктуации. Для начала откроем эти файлы и прочитаем данные. **Важно:** если в будущем файлы будут большие, то их не следует выводить на экран полностью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8178117c-2ab2-4f15-b353-61957f2a193a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a story about dogs\n",
      "our canine pets\n",
      "Dogs are furry animals\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('One.txt') as mytext:\n",
    "    print(mytext.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ed8951-22e4-4012-8a22-669e82e65d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This story is about surfing\n",
      "Catching waves is fun\n",
      "Surfing is a popular water sport\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('Two.txt') as mytext:\n",
    "    print(mytext.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46012c2-9323-496d-a009-57f2eb493c60",
   "metadata": {},
   "source": [
    "### Читаем весь текст как строку string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a365d510-3824-43e1-ba8b-865ee486e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('One.txt') as mytext:\n",
    "    a = mytext.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f764b2b9-f34c-4087-bca3-dc5c9ecc9136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a story about dogs\\nour canine pets\\nDogs are furry animals\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c76c6695-bf02-4801-af26-4821e9341979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a story about dogs\n",
      "our canine pets\n",
      "Dogs are furry animals\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a41a9e8-23f1-43f8-a178-230c51f53cfa",
   "metadata": {},
   "source": [
    "### Читаем каждую строку файла отдельно и помещаем в список"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "735cede0-ef0d-4474-9666-f76ef9221263",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('One.txt') as mytext:\n",
    "    lines = mytext.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "436d6df6-4ae5-4b38-a6f0-1ef74704c75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a story about dogs\\n',\n",
       " 'our canine pets\\n',\n",
       " 'Dogs are furry animals\\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd72f7-7605-45f2-ab33-7cfb33f7fffb",
   "metadata": {},
   "source": [
    "### Читаем отдельные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb567489-9b4e-4a00-a129-0aa812081235",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('One.txt') as f:\n",
    "    words = f.read().lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "050c5dcf-ad7a-47e5-885f-1de5ac6299ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'story',\n",
       " 'about',\n",
       " 'dogs',\n",
       " 'our',\n",
       " 'canine',\n",
       " 'pets',\n",
       " 'dogs',\n",
       " 'are',\n",
       " 'furry',\n",
       " 'animals']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe0565-ae2c-4d63-a986-875759289f27",
   "metadata": {},
   "source": [
    "## Создаём словарь vocabulary (Мешок слов - \"Bag of Words\")\n",
    "Для этого мы возьмём все слова из обоих документов, найдём уникальные слова, и пронумеруем их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "010c98ec-417a-4227-b628-bc41704b9a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('One.txt') as mytext:\n",
    "    words_one = mytext.read().lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbb9f5f6-278c-4c0e-9963-94d91a7f8643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'story',\n",
       " 'about',\n",
       " 'dogs',\n",
       " 'our',\n",
       " 'canine',\n",
       " 'pets',\n",
       " 'dogs',\n",
       " 'are',\n",
       " 'furry',\n",
       " 'animals']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c55a5d6-75e2-4095-bb3e-1653ab9bcd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3925d05-9903-4a7c-9dce-15100391bdc1",
   "metadata": {},
   "source": [
    "**Получим набор уникальных значений**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a3ceb3b-9d18-46e5-b32a-9763d728a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_words_one = set(words_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ec2be39-4464-4cc6-bbe9-851caf8a3a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'animals',\n",
       " 'are',\n",
       " 'canine',\n",
       " 'dogs',\n",
       " 'furry',\n",
       " 'is',\n",
       " 'our',\n",
       " 'pets',\n",
       " 'story',\n",
       " 'this'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_words_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f73d2287-5b00-4247-933c-e22575271122",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Two.txt') as mytext:\n",
    "    words_two = mytext.read().lower().split()\n",
    "    uni_words_two = set(words_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3411443d-9efc-4968-8922-8465f2e01ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'catching',\n",
       " 'fun',\n",
       " 'is',\n",
       " 'popular',\n",
       " 'sport',\n",
       " 'story',\n",
       " 'surfing',\n",
       " 'this',\n",
       " 'water',\n",
       " 'waves'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_words_two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73fe8c-1eb1-4f4e-964f-ca8c5f85594c",
   "metadata": {},
   "source": [
    "**Получаем все уникальные слова из всех документов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd5a1531-6904-4d7d-a260-75060ec336ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_uni_words = set()\n",
    "all_uni_words.update(uni_words_one)\n",
    "all_uni_words.update(uni_words_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0af55a5-f22c-4bb6-b54c-55f372f353a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'animals',\n",
       " 'are',\n",
       " 'canine',\n",
       " 'catching',\n",
       " 'dogs',\n",
       " 'fun',\n",
       " 'furry',\n",
       " 'is',\n",
       " 'our',\n",
       " 'pets',\n",
       " 'popular',\n",
       " 'sport',\n",
       " 'story',\n",
       " 'surfing',\n",
       " 'this',\n",
       " 'water',\n",
       " 'waves'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_uni_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f49e0d-e4ac-4087-a30b-857e69f66afd",
   "metadata": {},
   "source": [
    "**Пронумеруем все слова.**\n",
    "\n",
    "В цикле пройдём по всем словам из `all_uni_words`. Для каждого слова добавим элемент в словарь `full_vocab`, где ключом будет само слово, а значением - очередное число."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ceabcec8-4bd1-4dcb-95d5-d0a1553f8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab = {}\n",
    "i=0\n",
    "\n",
    "for word in all_uni_words:\n",
    "    full_vocab[word] = i\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe5b565e-be95-44f8-9a14-8583e0b04e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'story': 0,\n",
       " 'pets': 1,\n",
       " 'furry': 2,\n",
       " 'about': 3,\n",
       " 'animals': 4,\n",
       " 'our': 5,\n",
       " 'water': 6,\n",
       " 'waves': 7,\n",
       " 'dogs': 8,\n",
       " 'is': 9,\n",
       " 'are': 10,\n",
       " 'popular': 11,\n",
       " 'catching': 12,\n",
       " 'sport': 13,\n",
       " 'fun': 14,\n",
       " 'canine': 15,\n",
       " 'surfing': 16,\n",
       " 'a': 17,\n",
       " 'this': 18}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f96353-b541-49f5-b532-5fae162b05a3",
   "metadata": {},
   "source": [
    "**Множества set() - не являются отсортированными! Поэтому порядок элементов разный.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e3596c-1e56-4670-b0e5-ec737aa10ee3",
   "metadata": {},
   "source": [
    "## Мешок слов - считаем, как часто встречаются отдельные слова\n",
    "После того, как словарь из слов создан, выполним извлечение признаков для каждого из двух исходных документов.\n",
    "\n",
    "**Создаём пустые счётчики для каждого документа**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cce583c8-6807-442f-8d13-87aa81ab4f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список будет содержать столько нулей,\n",
    "# сколько элементов содержится в словаре full_vocab\n",
    "one_freq = [0] * len(full_vocab)\n",
    "two_freq = [0] * len(full_vocab)\n",
    "\n",
    "# В этом списке вместо 0 будут пустые строковые значения\n",
    "all_words = [''] * len(full_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4ebc903-708a-4569-b7c9-09e7831dc226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45224ec6-8a60-40cc-9f87-c667d9facbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7aec7251-4a37-4199-96fa-5263d8fc87ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f05e2bb-8f4a-4a63-b179-f0de0767e0ea",
   "metadata": {},
   "source": [
    "**Добавляем счётчики слов для каждого документа**\n",
    "\n",
    "Теперь пройдём по файлу One.txt и для каждого слова будем находить соответствующий этому слову номер в словаре `full_vocab`, а далее, будем увеличивать на единицу счётчик в списке `one_freq`. Таким образом будем подсчитывать сколько раз то или иное слово встречается в файле One.txt.\n",
    "\n",
    "Затем выполним всё то же самое для файла Two.txt, после чего объединим все слова из обоих файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "556db3b1-2803-4fe6-a4f9-31b3306d3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('One.txt') as f:\n",
    "    one_text = f.read().lower().split()\n",
    "\n",
    "for word in one_text:\n",
    "    # Для каждого слова найдём его индекс в словаре\n",
    "    word_ind = full_vocab[word]\n",
    "    one_freq[word_ind] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e330a31f-fe71-4611-abfd-5512da4f1a11",
   "metadata": {},
   "source": [
    "В переменной `one_text` содержится набор слов из файла One.txt. В цикле, мы берём каждое из этих слов и находим это слово в словаре `full_vocab`. Далее в списке со счётчиком увеличивается значение счётчика, соответствующего номеру текущего слова в словаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eaa9f5cf-b012-417a-93d3-bdd71a342603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 0, 0, 2, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cacade0a-526d-4795-9838-8a0e453d933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Two.txt') as f:\n",
    "    two_text = f.read().lower().split()\n",
    "\n",
    "for word in two_text:\n",
    "    # Для каждого слова найдём его индекс в словаре\n",
    "    word_ind = full_vocab[word]\n",
    "    two_freq[word_ind] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d4db863b-df86-42b3-8a4e-cbafdf97c75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1, 0, 0, 1, 1, 0, 3, 0, 1, 1, 1, 1, 0, 2, 1, 1]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ca62c-b640-4219-b127-7f2d70b64c6a",
   "metadata": {},
   "source": [
    "**Мы получили счётчики слов для двух файлов. Теперь можно создать список со всеми словами.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5c9044c-3f04-49e3-96fc-5098a67fe19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in full_vocab:\n",
    "     word_ind = full_vocab[word]\n",
    "     # Здесь уже не увеличение счётчика, а просто записываем слово\n",
    "     all_words[word_ind] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "47a630be-3198-49c9-8426-40346ae04773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['story',\n",
       " 'pets',\n",
       " 'furry',\n",
       " 'about',\n",
       " 'animals',\n",
       " 'our',\n",
       " 'water',\n",
       " 'waves',\n",
       " 'dogs',\n",
       " 'is',\n",
       " 'are',\n",
       " 'popular',\n",
       " 'catching',\n",
       " 'sport',\n",
       " 'fun',\n",
       " 'canine',\n",
       " 'surfing',\n",
       " 'a',\n",
       " 'this']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4fbdde8-f2f4-4909-b5e7-c1ebc171a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = pd.DataFrame(data=[one_freq, two_freq], columns=all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2e38fe89-0671-4981-a830-118acc345160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>pets</th>\n",
       "      <th>furry</th>\n",
       "      <th>about</th>\n",
       "      <th>animals</th>\n",
       "      <th>our</th>\n",
       "      <th>water</th>\n",
       "      <th>waves</th>\n",
       "      <th>dogs</th>\n",
       "      <th>is</th>\n",
       "      <th>are</th>\n",
       "      <th>popular</th>\n",
       "      <th>catching</th>\n",
       "      <th>sport</th>\n",
       "      <th>fun</th>\n",
       "      <th>canine</th>\n",
       "      <th>surfing</th>\n",
       "      <th>a</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   story  pets  furry  about  animals  our  water  waves  dogs  is  are  \\\n",
       "0      1     1      1      1        1    1      0      0     2   1    1   \n",
       "1      1     0      0      1        0    0      1      1     0   3    0   \n",
       "\n",
       "   popular  catching  sport  fun  canine  surfing  a  this  \n",
       "0        0         0      0    0       1        0  1     1  \n",
       "1        1         1      1    1       0        2  1     1  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14178f09-05d3-4202-a06c-692c363ab21e",
   "metadata": {},
   "source": [
    "Некоторые слова есть в обоих документах, но также есть слова которые находятся только в One.txt, а другие только в Two.txt. Если применить эту логику для десятков тысяч документов, то наш словарь вполне сможет вырасти до десятков тысяч слов. При этом матрицы будут содержать очень много нулей - это будут разреженные метрицы (sparse matrices).\n",
    "\n",
    "# Продолжение:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef34f4e2-75dc-43ca-b878-e9a84aef3efb",
   "metadata": {},
   "source": [
    "## Мешок слов и Tf-idf\n",
    "В приведённом выше примере, каждый вектор можно рассматривать как мешок слов (*bag of words*). Сами по себе эти вектора не очень полезны, но мы также можем добавить к ним частоту слов (*term frequencies - TF*) - как часто то или иное слово встречается в документе. Простой способ сделать это - это посчитать, сколько раз встречается слово в документе, и разделить на общее количество слов в документе. Тогда можно сравнивать между большими и маленькими документами, сколько раз то или иное слово встречается в документе.\n",
    "\n",
    "Однако, если какое-то слово встречается во многих документах, то такое слово не будет являться хорошим признаком для отделения документов друг от друга. Чтобы работать с такими словами, можно добавить метрику *inverse document frequency (IDF)*, которая вычисляется как общее количество документов, разделить на количество документов, в которых содержится рассматриваемое нами слово (слово, для которого вычисляется IDF). В практических задачах это значение приводится к логарифмической шкале, подробнее см. [здесь](https://ru.wikipedia.org/wiki/TF-IDF).\n",
    "\n",
    "Соединяя метрики TF (Term Frequency) и IDF (Inverse Document Frequency), мы получаем метрику [**tf-idf**](https://ru.wikipedia.org/wiki/TF-IDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d65d3-0ec1-405e-b7f8-5d95bdc53e16",
   "metadata": {},
   "source": [
    "## Стоп-слова и морфология слов\n",
    "Некоторые слова встречаются слишком часто, например слова \"the\" и \"and\" в английском языке. Эти слова можно просто исключить из рассмотрения. \n",
    "\n",
    "Кроме этого, одно и то же слово может встречаться в единственном и множественном числе, а также в различных падежах. Нам бы хотелось записать это слово один раз, а не записывать различные его вариации (например, записать только `cat` для обоих слов `cat` и `cats`). Это позволит нам уменьшить размер словаря и увеличить скорость работы модели.\n",
    "\n",
    "С другой стороны, различные падежи слов - особенно в русском языке - несут дополнительную смысловую информацию. Эту информацию, по-хорошему, можно сохранять, то есть ссылаться на базовое слово в словаре, плюс сохранять дополнительные атрибуты этого слова в конкретном месте текста. В данном блокноте мы ограничимся лишь базовым подсчётом частоты слов - как в отдельном документе, так и в целом наборе документов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb7707-5791-48f4-89a0-596de32ef851",
   "metadata": {},
   "source": [
    "# Часть 2: извлечение признаков и текста в Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925de382-d1a4-4c72-9014-248fd44f1f58",
   "metadata": {},
   "source": [
    "# Варианты извлечения признаков в Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "43793f02-3504-42af-9e2f-af09f812b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"This is a line\",\n",
    "        \"This is another line\",\n",
    "        \"Completely different line\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55fb75f-1987-4ae0-afd3-051726992ce0",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eacd26af-f1f8-4074-a750-2199e2e6141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "76c421fd-4e6f-42a7-964d-2f33e241f5b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
      " |  CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |\n",
      " |  Convert a collection of text documents to a matrix of token counts.\n",
      " |\n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.csr_matrix.\n",
      " |\n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |\n",
      " |  For an efficiency comparison of the different feature extractors, see\n",
      " |  :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : {'filename', 'file', 'content'}, default='content'\n",
      " |      - If `'filename'`, the sequence passed as an argument to fit is\n",
      " |        expected to be a list of filenames that need reading to fetch\n",
      " |        the raw content to analyze.\n",
      " |\n",
      " |      - If `'file'`, the sequence items must have a 'read' method (file-like\n",
      " |        object) that is called to fetch the bytes in memory.\n",
      " |\n",
      " |      - If `'content'`, the input is expected to be a sequence of items that\n",
      " |        can be of type string or byte.\n",
      " |\n",
      " |  encoding : str, default='utf-8'\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |\n",
      " |  decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |\n",
      " |  strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
      " |      Remove accents and perform other character normalization\n",
      " |      during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      a direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) means no character normalization is performed.\n",
      " |\n",
      " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      " |      :func:`unicodedata.normalize`.\n",
      " |\n",
      " |  lowercase : bool, default=True\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |\n",
      " |  preprocessor : callable, default=None\n",
      " |      Override the preprocessing (strip_accents and lowercase) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer`` is not callable.\n",
      " |\n",
      " |  tokenizer : callable, default=None\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |\n",
      " |  stop_words : {'english'}, list, default=None\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |      There are several known issues with 'english' and you should\n",
      " |      consider an alternative (see :ref:`stop_words`).\n",
      " |\n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |\n",
      " |      If None, no stop words will be used. In this case, setting `max_df`\n",
      " |      to a higher value, such as in the range (0.7, 1.0), can automatically detect\n",
      " |      and filter stop words based on intra corpus document frequency of terms.\n",
      " |\n",
      " |  token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |\n",
      " |      If there is a capturing group in token_pattern then the\n",
      " |      captured group content, not the entire match, becomes the token.\n",
      " |      At most one capturing group is permitted.\n",
      " |\n",
      " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      word n-grams or char n-grams to be extracted. All values of n such\n",
      " |      such that min_n <= n <= max_n will be used. For example an\n",
      " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
      " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
      " |      Only applies if ``analyzer`` is not callable.\n",
      " |\n",
      " |  analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      " |      Whether the feature should be made of word n-gram or character\n",
      " |      n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |\n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |\n",
      " |      .. versionchanged:: 0.21\n",
      " |\n",
      " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      " |      first read from the file and then passed to the given callable\n",
      " |      analyzer.\n",
      " |\n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |\n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |\n",
      " |  max_features : int, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      `max_features` ordered by term frequency across the corpus.\n",
      " |      Otherwise, all features are used.\n",
      " |\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |\n",
      " |  vocabulary : Mapping or iterable, default=None\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |\n",
      " |  binary : bool, default=False\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |\n",
      " |  dtype : dtype, default=np.int64\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |\n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |\n",
      " |  fixed_vocabulary_ : bool\n",
      " |      True if a fixed vocabulary of term to indices mapping\n",
      " |      is provided by the user.\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  HashingVectorizer : Convert a collection of text documents to a\n",
      " |      matrix of token counts.\n",
      " |\n",
      " |  TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
      " |      of TF-IDF features.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      " |  >>> corpus = [\n",
      " |  ...     'This is the first document.',\n",
      " |  ...     'This document is the second document.',\n",
      " |  ...     'And this is the third one.',\n",
      " |  ...     'Is this the first document?',\n",
      " |  ... ]\n",
      " |  >>> vectorizer = CountVectorizer()\n",
      " |  >>> X = vectorizer.fit_transform(corpus)\n",
      " |  >>> vectorizer.get_feature_names_out()\n",
      " |  array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
      " |         'this'], ...)\n",
      " |  >>> print(X.toarray())\n",
      " |  [[0 1 1 1 0 0 1 0 1]\n",
      " |   [0 2 0 1 0 1 1 0 1]\n",
      " |   [1 0 0 1 1 0 1 1 1]\n",
      " |   [0 1 1 1 0 0 1 0 1]]\n",
      " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
      " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
      " |  >>> vectorizer2.get_feature_names_out()\n",
      " |  array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
      " |         'second document', 'the first', 'the second', 'the third', 'third one',\n",
      " |         'this document', 'this is', 'this the'], ...)\n",
      " |   >>> print(X2.toarray())\n",
      " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      _VectorizerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __sklearn_tags__(self)\n",
      " |\n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which generates either str, unicode or file objects.\n",
      " |\n",
      " |      y : None\n",
      " |          This parameter is ignored.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted vectorizer.\n",
      " |\n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return document-term matrix.\n",
      " |\n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which generates either str, unicode or file objects.\n",
      " |\n",
      " |      y : None\n",
      " |          This parameter is ignored.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |\n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Transformed feature names.\n",
      " |\n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays of shape (n_samples,)\n",
      " |          List of arrays of terms.\n",
      " |\n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |\n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which generates either str, unicode or file objects.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _VectorizerMixin:\n",
      " |\n",
      " |  build_analyzer(self)\n",
      " |      Return a callable to process input data.\n",
      " |\n",
      " |      The callable handles preprocessing, tokenization, and n-grams generation.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      analyzer: callable\n",
      " |          A function to handle preprocessing, tokenization\n",
      " |          and n-grams generation.\n",
      " |\n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      preprocessor: callable\n",
      " |            A function to preprocess the text before tokenization.\n",
      " |\n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      tokenizer: callable\n",
      " |            A function to split a string into a sequence of tokens.\n",
      " |\n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols.\n",
      " |\n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc : bytes or str\n",
      " |          The string to decode.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      doc: str\n",
      " |          A string of unicode symbols.\n",
      " |\n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      stop_words: list or None\n",
      " |              A list of stop words.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _VectorizerMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  __sklearn_clone__(self)\n",
      " |\n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |\n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |\n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  __init_subclass__(**kwargs)\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |\n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |\n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b00ef2-273b-49a7-8835-1d89ba12e3ba",
   "metadata": {},
   "source": [
    "`CountVectorizer` - подсчитывает, сколько раз то или иное слово встречается в документе. \n",
    "\n",
    "Параметр `stop_words` позволяет удалить из рассмотрения слишком часто встречающиеся слова. По умолчанию установлено значение None, можно передать как например *english*, так и список собственных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c0cb9707-c6b1-4b90-838b-eaab7c3a0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "89a8bd62-7482-4474-a322-b866578020f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 10 stored elements and shape (3, 6)>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5673b17-6830-4471-904e-a05f3c517807",
   "metadata": {},
   "source": [
    "В данном случае, каждая из трёх строк будет обрабатываться как отдельный документ. На выходе мы получаем разреженную матрицу размером 3 на 6 (18 элементов), в которой находится 10 **ненулевых** чисел в сжатом формате. Здесь 3 - это количество документов, в нашем случае - это три строки, а 6 - это количество различных слов во всех этих документах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9905d939-d208-4bf4-8262-e76ee26520cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c0e1f7-8b52-46aa-a87c-7df5f914827e",
   "metadata": {},
   "source": [
    "**Далее эту матрицу можно преобразовать из разреженной в обычную с помощью метода `todense()`.**\n",
    "\n",
    "**Важно: при работе с большими разреженными матрицами - лучше не запускать метод `todense()`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f7d033fa-325c-42e2-97a4-6df4dcd4e8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 1, 1, 1],\n",
       "        [1, 0, 0, 1, 1, 1],\n",
       "        [0, 1, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f145e6a-d404-4b41-b3d4-c1c82d720ef4",
   "metadata": {},
   "source": [
    "**Посмотрим словарь слов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "72e19416-a21b-4527-9cd4-ec5306d4ddb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 5, 'is': 3, 'line': 4, 'another': 0, 'completely': 1, 'different': 2}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621154bd-8533-4027-b0f4-2119d0608e7c",
   "metadata": {},
   "source": [
    "Это такой же словарь, как и тот, что был создан нами вручную ранее. Например, для слова *another* - индекс 0, встречается оно только во втором документе - единица во втором списке на первой(нулевой) позиции, а в первой и третьей строке для этого слова указаны 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9310f6d9-52fa-4e72-9373-16222b976a01",
   "metadata": {},
   "source": [
    "## Стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0dd7d3bf-31ee-4e23-ab61-ed4bedd21f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b1b2a947-f5fa-4b9c-865e-36821ccf2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "183dde97-e9b7-4d71-889e-3aa8e20948c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 1, 1]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f1cc698b-b55b-4843-8b0c-33cbdc2e99ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'line': 2, 'completely': 0, 'different': 1}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87142854-3097-4f7b-91ca-b3ad12cd9cf4",
   "metadata": {},
   "source": [
    "Мы получили матрицу размером 3 на 3. В словаре также осталось только 3 слова. Из рассмотрения были исключены такие слова как: this, is, another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8eec71-b98e-445a-ace5-d4de27e8b69f",
   "metadata": {},
   "source": [
    "## TfidfTransformer\n",
    "\n",
    "Посмотрим как можно преобразовать данные с помощью TF-IDF, т.е. не только подсчитать количество слов с помощью TF, но и с помощью IDF учесть то, насколько часто слова встречаются во всём наборе документов.\n",
    "\n",
    "`TfidfVectorizer` применяется **для текстовых документов**, а `TfidfTransformer` применяется **к матрице со счётчиками, которую возвращает CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b80e1737-9938-485d-bfb7-d8e6f7a491c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "568903f5-5a77-49be-b634-2457b52a76b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c5ca58-9aa9-4c6e-9ea1-e74baa83ac17",
   "metadata": {},
   "source": [
    "**Рассмотрим полный словарь из 6 слов, чтобы было нагляднее**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d56d4375-d48c-48e5-8cf8-ef3cc944a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "51268bef-c3c5-4a93-8fb8-bf8e05810ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "db83f4ae-cb3b-4473-8b28-796949fbc984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 10 stored elements and shape (3, 6)>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5304a134-94b3-4ca0-88f9-0cb6f558d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tfidf.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b10fbbb-4202-4d1f-a947-48411fc7e5e9",
   "metadata": {},
   "source": [
    "Здесь мы на вход подаём мешок слов, а на выходе получаем значение TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f4d48d51-46f4-409b-98b2-d6d2ab7bc97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 10 stored elements and shape (3, 6)>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b870646a-4f5b-4592-aac4-9b5253d59465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.61980538, 0.48133417,\n",
       "         0.61980538],\n",
       "        [0.63174505, 0.        , 0.        , 0.4804584 , 0.37311881,\n",
       "         0.4804584 ],\n",
       "        [0.        , 0.65249088, 0.65249088, 0.        , 0.38537163,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe7d31-beca-42df-aa0e-ffc44c951b7c",
   "metadata": {},
   "source": [
    "Мы получили счётчики TF-IDF, но уже в два шага. Сначала был запущен `CountVectorizer`, а затем `TfidfTransformer`. Но эти два шага можно объединить в один, с помощью `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af1a17-426b-4805-81d2-fe9eb1a186d7",
   "metadata": {},
   "source": [
    "## TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "124c72bf-0bce-400a-ba89-c320b1af9860",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "75023c26-fdac-4aaa-9016-d267921272ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 10 stored elements and shape (3, 6)>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "73545e0a-c6ef-4166-8fd8-21f7da0b4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_results = tv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a2e2a19a-6602-46ac-a397-08a5dc3511a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.61980538, 0.48133417,\n",
       "         0.61980538],\n",
       "        [0.63174505, 0.        , 0.        , 0.4804584 , 0.37311881,\n",
       "         0.4804584 ],\n",
       "        [0.        , 0.65249088, 0.65249088, 0.        , 0.38537163,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_results.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34df294-e4c2-42f3-9280-cef1108eab27",
   "metadata": {},
   "source": [
    "`TfidfVectorizer` возвращает уже готовые результаты. Однако иногда может возникнуть вопрос по отдельным элементам, почему результаты получились именно такими, какими получились - например, подозрительно большими или подозрительно маленькими. Тогда можно использовать промежуточные методы - прежде всего `CountVectorizer` - чтобы посмотреть промежуточные результаты и понять, как именно получились результаты для `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "84f4c0a1-3f7b-4ba0-863b-6499921930ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = {k: v for k, v in sorted(tv.vocabulary_.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0c5bc1d9-b572-4808-af59-39464fb291f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>another</th>\n",
       "      <th>completely</th>\n",
       "      <th>different</th>\n",
       "      <th>is</th>\n",
       "      <th>line</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.619805</td>\n",
       "      <td>0.481334</td>\n",
       "      <td>0.619805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.631745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480458</td>\n",
       "      <td>0.373119</td>\n",
       "      <td>0.480458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    another  completely  different        is      line      this\n",
       "0  0.000000    0.000000   0.000000  0.619805  0.481334  0.619805\n",
       "1  0.631745    0.000000   0.000000  0.480458  0.373119  0.480458\n",
       "2  0.000000    0.652491   0.652491  0.000000  0.385372  0.000000"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tv_results.todense(), columns=words_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
